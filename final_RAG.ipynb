{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf9n5omIOe56",
        "outputId": "c615db4a-c3b9-4a05-e5d5-6c091c57ff81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/23.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.9/23.7 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.9/23.7 MB\u001b[0m \u001b[31m126.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17.8/23.7 MB\u001b[0m \u001b[31m226.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m288.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/328.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m328.2/328.2 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m476.1/476.1 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q faiss-cpu sentence-transformers transformers accelerate pypdf gradio langchain langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loaders de documents\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Embeddings + Vector DB\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# LLM HuggingFace\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "# Utilitaires\n",
        "from pathlib import Path\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "F8EKQWvSPRr6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dossier contenant vos PDFs\n",
        "DATA_DIR = \"/content/cours\"\n",
        "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "Nwx3p1aLPcHl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_documents(folder):\n",
        "    \"\"\"Charge tous les PDFs du dossier.\"\"\"\n",
        "    docs = []\n",
        "    for file in Path(folder).glob(\"**/*.pdf\"):\n",
        "        loader = PyPDFLoader(str(file))\n",
        "        docs.extend(loader.load())\n",
        "    return docs\n",
        "\n",
        "# Charger les documents\n",
        "documents = load_all_documents(DATA_DIR)\n",
        "print(f\"\ud83d\udcc4 {len(documents)} pages charg\u00e9es\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXDdq-8fPuWM",
        "outputId": "27a5782e-bcc9-4831-c50b-948c13c5b82a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcc4 206 pages charg\u00e9es\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_documents(docs, chunk_size=800, overlap=150):\n",
        "    \"\"\"D\u00e9coupe les documents en chunks avec chevauchement.\"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    for doc in docs:\n",
        "        text = doc.page_content\n",
        "        start = 0\n",
        "\n",
        "        while start < len(text):\n",
        "            chunk_text = text[start:start + chunk_size]\n",
        "            # Cr\u00e9er un nouveau document avec le m\u00eame type que l'original\n",
        "            chunk_doc = type(doc)(\n",
        "                page_content=chunk_text,\n",
        "                metadata=doc.metadata\n",
        "            )\n",
        "            chunks.append(chunk_doc)\n",
        "            start += chunk_size - overlap\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# D\u00e9couper les documents\n",
        "chunks = split_documents(documents)\n",
        "print(f\"\u2702\ufe0f {len(chunks)} chunks cr\u00e9\u00e9s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyfdOnTEP0G6",
        "outputId": "5c571c39-d4b9-405c-8fa4-d5f31bef2625"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2702\ufe0f 176 chunks cr\u00e9\u00e9s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mod\u00e8le d'embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "\n",
        "# Cr\u00e9ation de la base FAISS\n",
        "vectordb = FAISS.from_documents(chunks, embeddings)\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "print(\"\u2705 Base vectorielle cr\u00e9\u00e9e\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545,
          "referenced_widgets": [
            "6c9a74a854d342e386bfc10447919b55",
            "74baaa8e1b184ecda1022ebda4d095b3",
            "4df63237ac03402aac944830264b4398",
            "31b7c195d2184689ba234862e005e1d4",
            "658dd980f2ca498eb2287d25b4755f38",
            "630728df4a71480f94fbdd3423394eee",
            "ff866771f3a94e8f96028d67841d97b2",
            "26eb02d07e3143da8197565414c9cc66",
            "8eac781ac8c249f8a75984a9dc2fb8ad",
            "c60814a93dd1493bbb767952f230045e",
            "c678cf1664bf40cf947d2642a5c80cf6",
            "dcb4c1a086f34c1b90d50fa0fe65aa77",
            "12e25cbad7f64cb6be7ca032b7d08bc4",
            "0fd466f0eb314ab09859b8d8d520a3ba",
            "24263b9fb0c4463c9eb6509ecdc84fa2",
            "c7f1f05d007e42f3991a7f2a23ed2926",
            "86bf78a1b2e7464e8df7e9ef2157ca41",
            "623174061dd84b8ca64eaa734621bde3",
            "534da63d822c464597975fc00cad0365",
            "2b41f9ef625b4d9eb326f83e246147dd",
            "35315c2d2e2a46e9ba39609ef534cb44",
            "4c4c4f4bc8a1482fba3f55930588efa6",
            "bdaf49131d1743e4ae3a3055c45a488f",
            "370c949186be4166a5fbfcf999ff342d",
            "37e232b948804e458ffa463fd2b165de",
            "c8f91ccd6486455daeed1e4143819642",
            "532208815eea46c3a9de6f5d89fbd474",
            "dbba14812c8d46a5834b1511db45591d",
            "0d93070a2e5e4afd8960e784cf4b2723",
            "12dbe0d0460c44619ef9c00ae3361b66",
            "665a54155a89410c8f12e9f8883aff87",
            "e95588bd7cb84ea1b42cd73c954117f1",
            "cd5064d080364330bc608f47ca9497b2",
            "1bb25651facb4069811220fecde2fb48",
            "02a8ce54b23541899fe44fe244d5c8d2",
            "1282f30af7074bc8b321d44506bc82ba",
            "7a4bd9c98e114d0183e1618b4c962e86",
            "9935e2419db54037956d123e025347d3",
            "15bb2bb744764cf1962b8445c95b5726",
            "fc198414816f44bab16e22d35823fb02",
            "82f05540d72245f99a2b141783356c23",
            "b1b96f860b06417ba936fd6c595a93ae",
            "ca82c99a5048481a8150d7bd446c55a5",
            "34838acb49b64c78ab31150a536f3f78",
            "c2def55b23eb4d5b90bf003ed9810db1",
            "3e6aa91e51da448a97ee08f18d476d37",
            "b04fa710946948e6988dd42e19474af3",
            "979bb38456db42bc9220c8aa956ad1c8",
            "a7fe3d3466a14cccb12d8afaa6ed715c",
            "40819317e6d44a04b0ae88df1903f40f",
            "8cfbb4ffbacb4eccbc3ceca194eaed2e",
            "c7c6f499f0bd495fa6a15ae9ada005b6",
            "b3091b28bd104491859f7662e58fb0a9",
            "3a9ad654122743afaf25f5bff7770569",
            "e42c565c881e466faf7f40e2457bd5b7",
            "6fd7b27a0ee644cb98b10214259d77ea",
            "3b736a446200435eabd38ac74f53a65e",
            "2d8258d7ad5340b0a92f78e1f3449a69",
            "0b18c650251b44e19bf8d2c0ac70e577",
            "785c86286d0c4a11bd0a25602666107a",
            "babbb6e467c640df89556e31f43f7273",
            "e3608767276648018907f6673ff0d827",
            "e2b07eaba91948378b0fc95784012b63",
            "e7380ad0d19544cbaac696137b59f59a",
            "e7a5413e59ad49ec998453bd06e526b0",
            "494416fb246a4fd4b16620b27e1aef0a",
            "92e1095350024904b608e056797d5c01",
            "83871553f32d468b970b108902670ebb",
            "ca0b1b6718fd4273a8a8a1bcfe3648aa",
            "c3234488a30d4d2da36d1cf8431c336c",
            "70a6efc03ded4ce9ba80298d1238d781",
            "9c60b903abb847c9a94f0d5544ec0235",
            "df49f94ac66d4f879ac99583ec3dc3ab",
            "49e9123e60e64dec9356d59f7a2aa1c9",
            "b1a049d2c44e4f52be8e8815a010d7f0",
            "057ad9c3728644dda4c886d40bbee5f4",
            "2fc016382a724680b74d893723bf4440",
            "876df804f2834a5e831af6569fc63c70",
            "fa4ef18552804c848f9e38ec423e6029",
            "9a57e0a4a58142a4b82145bb730dee7e",
            "370c50278b124ae2b4b10c558490142c",
            "a7ce023423824bc9b1ba2808ee8cb742",
            "593c376ef8864cdc9fad6284c71fa5c6",
            "304b89aa9dd94a0eab9936b097fe082f",
            "2d01bbaca1c947ed953e74729578eef5",
            "a67256d9a19b4601a84fa29bdc24707d",
            "a8f55dca8eb1435f8ff591f13629400b",
            "8bbdbd37cfd8493ebbb17e5ea5ad3127",
            "96579ccddcd44430945a4bff0a494b67",
            "effc8fd59d164c34933efe4f465cbb6d",
            "2ccac086396e4660b9216faa0f1b3a5e",
            "1447f0a7dc3d411ab9b7fe01dfbc82c3",
            "650a3f314da94b74a47275f5850c9d4f",
            "007f3f6c96b445a6a18592974e5a7bf8",
            "5b67707444ae4997ad844f75f08be251",
            "52b7c25ef3b34799aeacd5b9db76d7cd",
            "be001f0aab0d46ae83671412d92879dd",
            "bf2db93a3a4445fd8bbd21866a94f5bf",
            "4f6b47ecb3d04e8490e1c6a7d6881129",
            "aea5509a489048488256eef205fbd9d8",
            "5fe3e74c169b418593e220664bb0f6e0",
            "9f94072d65b548bea84ec5be7bd9a6b7",
            "aedc2a31f93d488cb7f75abde12daee7",
            "83839200eaf1424898a96f64f5a4e5b6",
            "d8eeed21da844108bdba4c027f65f132",
            "19befe8206d04af39ade64ee8ed94f35",
            "028fcf6071684ff69842162c583cdd4c",
            "b6918a65dab040588c1832c5417277ff",
            "d0fbdfe242474300b7deb3a1ba10fa3e",
            "cf9668833fed4be3b4794f7234453013",
            "f84703ebd0b342e68252a9eee6249662",
            "5b0f0be6202647c68ef5375dcf080c7e",
            "1482b677997d4623a44232c0799f1283",
            "4f222895bc534eee92f3d8d3517dfb91",
            "01e19cbf697742d59b03443650ab461c",
            "9e99ec6eb8b94057bdebd27215f595d6",
            "6ba8c8ba39d54885adfe4e817bea185f",
            "2dfef591c1b645a2866fa18fc78eeaf9",
            "565375dbd4ed4c079d5b3b50c65cb5a0",
            "d76ec34d6bd949a88ccfbd2232b3d827",
            "e9c10587f0c74477ad8f729e02f609ba"
          ]
        },
        "id": "v-lGxnsGP3DS",
        "outputId": "efe899ab-3070-4899-c2a9-130aef63f1d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-168259903.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c9a74a854d342e386bfc10447919b55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcb4c1a086f34c1b90d50fa0fe65aa77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdaf49131d1743e4ae3a3055c45a488f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bb25651facb4069811220fecde2fb48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2def55b23eb4d5b90bf003ed9810db1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fd7b27a0ee644cb98b10214259d77ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92e1095350024904b608e056797d5c01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "876df804f2834a5e831af6569fc63c70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96579ccddcd44430945a4bff0a494b67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aea5509a489048488256eef205fbd9d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f84703ebd0b342e68252a9eee6249662"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Base vectorielle cr\u00e9\u00e9e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_NAME = \"bigscience/bloom-560m\"\n",
        "\n",
        "# Charger le tokenizer et le mod\u00e8le\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(LLM_NAME, device_map=\"auto\")\n",
        "\n",
        "# Cr\u00e9er le pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.3,\n",
        "    top_p=0.95\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "print(\"\ud83e\udd16 Mod\u00e8le charg\u00e9\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "dfd7eef34ac24a9a933b255acda1da59",
            "a6ad193133a047be81778182038e7a5c",
            "dee7e5c3322e4156b143a5748d745053",
            "a6d18e05a68a455ba86397e66859ebb1",
            "ec4097224eac48aebab827729f0ac72e",
            "ffbf10d2df0b40619dac253c4bf8e3be",
            "7ca7d7f8baca4e6882349c64c35148a7",
            "4624a9decb3f4796ae02f1d14bf67496",
            "512060193e3244bc9ce7c518b01a593b",
            "0bab7d6bdab540d0b500003d088a0f13",
            "59e1d242e0284229a0958c0f3292d8a5",
            "8cd922e131db4aa3b946ac27cecd4af5",
            "6e8893031f0b4181891161bd7adc6e0a",
            "c2daaffbdcb44de3bf74794bed081293",
            "40ff1eda5177426b85a07401a10e5771",
            "47cc490fe77d405b968f69352cda2408",
            "a81c52b73c5a4db1bf2b7802ef3ff640",
            "7fe77a5aad294d40a17bf5136ffb6971",
            "9c6f4e1edd574ccf91b7e6bdb21bb5be",
            "2f924569db1147faac5d15c1be0e13e4",
            "1d675f28eb7a405dbb237b0414dbec1b",
            "51466d8578cc4d02a8faabcd8d549969",
            "da54eebfdb5e4f4e94929e61a4a63e46",
            "7eea823864dc4e81a3a14b33930f8bb7",
            "e377a990a1fc49c3b37a651f6f60e85f",
            "00baa63c2e2042dd93e783ede873ac0b",
            "06a197ac63c64ae9807a9cab19fdfd1b",
            "8436943b067f4171a9002ebc40054e6e",
            "5162ca067a5b49db89d15ef647ec5ed3",
            "c4f37393fcb744de80d853cd67d421ef",
            "3ed30234d4f944648ee98f84d7c7db54",
            "08496e259ff546af8e9331a9682e58b8",
            "ef8165d2c9764ba4a6e68aa2c4aab294",
            "935871b016eb401483750b622ded9f15",
            "20a2216708144ae9863bd3e80ebc3adf",
            "2249b4339683425ba082aaecb14a7793",
            "6674888d9c9141068a09ab5f73dc60ec",
            "26e57affead44ec5893be2ae2920c748",
            "1fc295a862f64b0fba60e93cb5bfa6b2",
            "16b422ee1b794f53897a1283637739b4",
            "3941dbcc099847a7b7b74682ca92dbd9",
            "9fec2711dec749628c5c40731cea9dcd",
            "0d14c464df1643fcad5719c9ae85ffd3",
            "16cd7b9d213f45f58abeabd22578fd24",
            "c7db765bd73c417cb2e90be393d98143",
            "0556221410e94d9a9d20145ad2a32662",
            "6dfe0ceaa8d14b2ab1c20d41b78e63e5",
            "dea00e821df9401dbcda3f3e845f46e3",
            "eb43f1a846f64fa3b860b467aed41ee4",
            "3dc9e4289d8a4a869703f43833f6c595",
            "f45c0bddc927412b87b30f3b40e87d2c",
            "dcd03312a24243f19651c1ea02b0d1e5",
            "e2529fc433434459a1c3d2c51335b8bb",
            "cb418092f3b24d8591e35fe9f626ea62",
            "c7466122e97747769893679b27dc7bd4"
          ]
        },
        "id": "CFq5dPdDQGpn",
        "outputId": "cc5d4fa4-778d-4aa4-f87b-67d734106b0b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfd7eef34ac24a9a933b255acda1da59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cd922e131db4aa3b946ac27cecd4af5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da54eebfdb5e4f4e94929e61a4a63e46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "935871b016eb401483750b622ded9f15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7db765bd73c417cb2e90be393d98143"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83e\udd16 Mod\u00e8le charg\u00e9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2242607605.py:17: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_rag_final(query, retriever, llm):\n",
        "    \"\"\"Version finale optimis\u00e9e.\"\"\"\n",
        "\n",
        "    # 1. R\u00e9cup\u00e9ration\n",
        "    docs = retriever.invoke(query)\n",
        "    if isinstance(docs, dict):\n",
        "        docs = docs.get(\"documents\", [])\n",
        "    if not docs:\n",
        "        return \"\u274c Aucune information trouv\u00e9e.\"\n",
        "\n",
        "    # 2. Contexte court\n",
        "    context = \"\\n\".join([\n",
        "        (d.page_content if hasattr(d, \"page_content\") else str(d))[:700]\n",
        "        for d in docs[:2]\n",
        "    ])\n",
        "\n",
        "    # 3. Prompt ultra-simple\n",
        "    prompt = f\"{context}\\n\\nQuestion: {query}\\nR\u00e9ponse courte:\"\n",
        "\n",
        "    # 4. G\u00e9n\u00e9ration\n",
        "    output = llm.invoke(prompt)\n",
        "    text = output[0][\"generated_text\"] if isinstance(output, list) else str(output)\n",
        "\n",
        "    # 5. Extraction\n",
        "    if \"R\u00e9ponse courte:\" in text:\n",
        "        text = text.split(\"R\u00e9ponse courte:\")[-1]\n",
        "\n",
        "    # Garder 2 premi\u00e8res phrases\n",
        "    sentences = [s.strip() for s in text.split(\".\") if len(s) > 15][:2]\n",
        "    return \". \".join(sentences) + \".\" if sentences else \"Information non disponible.\""
      ],
      "metadata": {
        "id": "BSuz91sfQOrC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_interface(user_msg, history):\n",
        "    \"\"\"Interface de chat pour Gradio.\"\"\"\n",
        "    if not user_msg.strip():\n",
        "        return history, history\n",
        "\n",
        "    try:\n",
        "        answer = ask_rag_final(user_msg, retriever, llm)\n",
        "    except Exception as e:\n",
        "        answer = f\"\u274c Erreur: {str(e)}\"\n",
        "\n",
        "    history.append((user_msg, answer))\n",
        "    return history, history\n",
        "\n",
        "\n",
        "# Interface Gradio\n",
        "with gr.Blocks(title=\"Chatbot RAG PDF\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # \ud83d\udcda Chatbot RAG - Questions sur vos Documents\n",
        "\n",
        "    Posez des questions sur vos documents PDF et obtenez des r\u00e9ponses pr\u00e9cises bas\u00e9es sur leur contenu.\n",
        "    \"\"\")\n",
        "\n",
        "    chatbot = gr.Chatbot(\n",
        "        height=500,\n",
        "        type=\"tuples\",\n",
        "        label=\"Conversation\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        msg_box = gr.Textbox(\n",
        "            label=\"Votre question\",\n",
        "            placeholder=\"Posez votre question ici...\",\n",
        "            scale=4\n",
        "        )\n",
        "        submit_btn = gr.Button(\"Envoyer\", scale=1, variant=\"primary\")\n",
        "\n",
        "    gr.Markdown(\"\ud83d\udca1 **Conseil:** Posez des questions pr\u00e9cises pour obtenir les meilleures r\u00e9ponses.\")\n",
        "\n",
        "    # Actions\n",
        "    msg_box.submit(\n",
        "        fn=chat_interface,\n",
        "        inputs=[msg_box, chatbot],\n",
        "        outputs=[chatbot, chatbot]\n",
        "    ).then(\n",
        "        lambda: \"\",\n",
        "        outputs=[msg_box]\n",
        "    )\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=chat_interface,\n",
        "        inputs=[msg_box, chatbot],\n",
        "        outputs=[chatbot, chatbot]\n",
        "    ).then(\n",
        "        lambda: \"\",\n",
        "        outputs=[msg_box]\n",
        "    )\n",
        "\n",
        "# Lancer l'interface\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "RGa3DagkQR-_",
        "outputId": "952ff2ce-dfd3-44e4-f312-d5d9d5bae395"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2243211113.py:16: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(title=\"Chatbot RAG PDF\", theme=gr.themes.Soft()) as demo:\n",
            "/tmp/ipython-input-2243211113.py:23: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-2243211113.py:23: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://215bac553a4f9c7416.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://215bac553a4f9c7416.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://215bac553a4f9c7416.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}